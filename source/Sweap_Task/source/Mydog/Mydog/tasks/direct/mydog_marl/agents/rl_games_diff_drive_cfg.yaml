params:
  # 1. 通用参数
  # 1.1 随机种子设置
  seed: 48 # 确保实验的可重复性

  # 2. 环境参数
  env:
    clip_actions: 1.0  # 动作值的裁剪范围，限制输出动作的幅度

  # 3. 算法选择
  algo:
    name: a2c_continuous  # 使用的强化学习算法，A2C (Advantage Actor-Critic)

  # 4. 模型参数
  model:
    # 4.1 网络类型
    name: continuous_a2c_logstd  # 连续动作空间的 A2C 模型

    # 4.2 网络结构
  network:
    name: actor_critic  # 使用 Actor-Critic 网络结构
    separate: False  # 是否使用独立的 actor 和 critic 网络
    rnn:
      name: gru
      units: 128
      layers: 1   # 可选，默认=1
    # 4.3 连续动作空间配置
    space:
      continuous:
        mu_activation: None  # 平均值输出层的激活函数
        sigma_activation: None  # 标准差输出层的激活函数

        # 4.3.1 平均值初始化
        mu_init:
          name: default  # 使用默认初始化方法

        # 4.3.2 标准差初始化
        sigma_init:
          name: const_initializer  # 使用常数初始化方法
          val: 0  # 初始化值设为0

        fixed_sigma: True  # 是否使用固定的标准差

      # 4.4 MLP 网络参数
    mlp:
      units: [128, 128]  # MLP 隐藏层神经元数量
      activation: elu  # 使用 ELU 激活函数
      d2rl: True  # 是否使用 D2RL 架构增强（Deep Dense ReLU）

      # 4.4.1 权重初始化
      initializer:
        name: default  # 使用默认初始化方法

      # 4.4.2 正则化
      regularizer:
        name: None  # 不使用正则化

  # 5. 检查点加载配置
  load_checkpoint: True # 是否从检查点加载模型
  load_path: '/home/astwea/MyDogTask/Mydog/logs/rl_games/diff_drive_direct/2025-06-03_12-58-53/nn/diff_drive_direct.pth'  # 检查点路径

  # 6. 训练配置
  config:
    # 6.1 基本设置
    name: diff_drive_direct  # 环境名称
    env_name: rlgpu  # 具体的环境类型
    device: 'cuda:0'  # 使用的计算设备
    device_name: 'cuda:0'  # 设备名称
    multi_gpu: False  # 是否使用多 GPU 训练
    ppo: True  # 使用 PPO 算法增强
    mixed_precision: True  # 是否使用混合精度训练
    normalize_input: False  # 是否归一化输入数据
    normalize_value: True  # 是否归一化价值函数输出
    value_bootstrap: True  # 是否使用价值引导

    # 6.2 并行设置
    num_actors: -1  # 环境实例数量，-1 表示自动配置

    # 6.3 奖励设置
    reward_shaper:
      scale_value: 1.0 # 奖励缩放系数

    # 6.4 策略优化参数
    normalize_advantage: True  # 归一化优势函数
    gamma: 0.99  # 折扣因子
    tau: 0.95  # GAE（广义优势估计）衰减因子
    learning_rate: 1e-4  # 学习率
    lr_schedule: adaptive  # 自适应学习率调度
    schedule_type: legacy  # 学习率调度类型
    kl_threshold: 0.01  # KL 散度阈值
    score_to_win: 20000  # 期望的最高分数（训练目标）
    max_epochs: 150000000  # 最大训练轮数
    save_best_after: 200  # 在指定轮次后保存最佳模型
    save_frequency: 100  # 每隔多少轮保存模型
    grad_norm: 1.0  # 梯度裁剪阈值
    entropy_coef: 0.007  # 熵奖励系数（鼓励探索）
    truncate_grads: True  # 是否裁剪梯度

    # 6.5 PPO 参数
    e_clip: 0.2  # PPO 剪辑范围
    horizon_length: 64  # PPO 轨迹长度
    minibatch_size: 32768  # 最小批量大小
    mini_epochs: 5  # 每个批量的训练轮次
    critic_coef: 2.0  # 价值函数损失系数
    clip_value: True  # 是否裁剪价值函数输出

    # 6.6 序列和边界损失
    seq_length: 4  # RNN 序列长度
    bounds_loss_coef: 0.0  # 边界损失系数（如果需要约束动作范围）

